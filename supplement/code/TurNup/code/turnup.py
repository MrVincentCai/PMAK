import numpy as np
import pickle
import pandas as pd
import os
from os.path import join
import warnings
import torch
import esm
from drfp import DrfpEncoder
from tqdm import tqdm
from sklearn.metrics import r2_score
from scipy import stats
import xgboost as xgb
import matplotlib.pyplot as plt
import matplotlib as mpl

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# -------------------------- å…¨å±€é…ç½® --------------------------
# æ•°æ®æ ¹è·¯å¾„ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
DATA_ROOT = "PMAK_"
# ç‰¹å¾ä¸æ¨¡å‹ä¿å­˜æ ¹è·¯å¾„
FEATURE_ROOT = join(DATA_ROOT, "features")
MODEL_ROOT = join(DATA_ROOT, "models")
# è®¾å¤‡é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ESMæ¨¡å‹é…ç½®
ESM_MODEL_NAME = "esm1b_t33_650M_UR50S"
MAX_SEQ_LENGTH = 1022  # ESMæ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦
BATCH_SIZE = 16  # ESMç‰¹å¾æå–æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ï¼‰
# ç›®æ ‡åˆ—å
TARGET_COL = "log10_kcat"
SOURCE_COL = "geomean_kcat"  # åŸå§‹æ•°æ®ä¸­çš„ç›®æ ‡åˆ—å

# åˆ›å»ºå¿…è¦ç›®å½•
os.makedirs(FEATURE_ROOT, exist_ok=True)
os.makedirs(MODEL_ROOT, exist_ok=True)


# -------------------------- 1. CSVè½¬PKLå·¥å…·å‡½æ•° --------------------------
def csv_to_pkl(csv_path, pkl_path):
    """å°†CSVæ–‡ä»¶è½¬æ¢ä¸ºPKLæ ¼å¼å¹¶ä¿å­˜"""
    if not os.path.exists(pkl_path):
        df = pd.read_csv(csv_path)
        # é‡å‘½åç›®æ ‡åˆ—ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if SOURCE_COL in df.columns and TARGET_COL not in df.columns:
            df.rename(columns={SOURCE_COL: TARGET_COL}, inplace=True)
        df.to_pickle(pkl_path)
        print(f"âœ… å·²å°†CSVè½¬æ¢ä¸ºPKL: {csv_path} â†’ {pkl_path}")
    else:
        print(f"â„¹ï¸ PKLæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è½¬æ¢: {pkl_path}")
    return pd.read_pickle(pkl_path)


# -------------------------- 2. ESMç‰¹å¾æå–å·¥å…·å‡½æ•° --------------------------
def load_esm_model():
    """åŠ è½½ESMæ¨¡å‹"""
    model, alphabet = esm.pretrained.load_model_and_alphabet(ESM_MODEL_NAME)
    model = model.eval().to(DEVICE)
    return model, alphabet


def extract_esm_features(df, seq_col="enzyme_sequence"):
    """ä¸ºDataFrameä¸­çš„åºåˆ—æå–ESMç‰¹å¾"""
    # æ£€æŸ¥åºåˆ—åˆ—æ˜¯å¦å­˜åœ¨
    if seq_col not in df.columns:
        raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘åºåˆ—åˆ—: {seq_col}")
    
    # åŠ è½½æ¨¡å‹
    model, alphabet = load_esm_model()
    batch_converter = alphabet.get_batch_converter()
    
    # å¤„ç†è¿‡é•¿åºåˆ—
    df[seq_col] = df[seq_col].apply(
        lambda x: x[:MAX_SEQ_LENGTH] if isinstance(x, str) and len(x) > MAX_SEQ_LENGTH else x
    )
    
    # æå–ç‰¹å¾
    sequences = df[seq_col].tolist()
    features = []
    
    for i in tqdm(range(0, len(sequences), BATCH_SIZE), desc="æå–ESMç‰¹å¾"):
        batch = sequences[i:i+BATCH_SIZE]
        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        batch_labels = [f"seq_{j}" for j in range(i, min(i+BATCH_SIZE, len(sequences)))]
        batch_data = list(zip(batch_labels, batch))
        _, _, batch_tokens = batch_converter(batch_data)
        batch_tokens = batch_tokens.to(DEVICE)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[33], return_contacts=False)
        token_reps = results["representations"][33]  # (batch_size, seq_len, 1280)
        
        # è®¡ç®—åºåˆ—çº§ç‰¹å¾ï¼ˆå¹³å‡æ± åŒ–ï¼‰
        for rep in token_reps:
            # æ’é™¤èµ·å§‹å’Œç»ˆæ­¢token
            seq_rep = rep[1:-1].mean(dim=0).cpu().numpy()
            features.append(seq_rep)
    
    return np.array(features)


# -------------------------- 3. DRFPç‰¹å¾æå–å·¥å…·å‡½æ•° --------------------------
def extract_drfp_features(df, smiles_col="reaction_smiles"):
    """ä¸ºDataFrameä¸­çš„SMILESæå–DRFPç‰¹å¾"""
    if smiles_col not in df.columns:
        raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘SMILESåˆ—: {smiles_col}")
    
    smiles_list = df[smiles_col].fillna("").tolist()
    # ç”ŸæˆDRFPæŒ‡çº¹ï¼ˆ2048ç»´ï¼‰
    fps, _ = DrfpEncoder.encode(smiles_list, nBits=2048)
    return fps


# -------------------------- 4. æ•°æ®é¢„å¤„ç†å®Œæ•´æµç¨‹ --------------------------
def preprocess_data(csv_train_path, csv_test_path, dataset_name, fold=None):
    """
    å®Œæ•´æ•°æ®é¢„å¤„ç†æµç¨‹ï¼šCSVâ†’PKLâ†’ESMç‰¹å¾â†’DRFPç‰¹å¾
    è¿”å›å¸¦ç‰¹å¾çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†PKLè·¯å¾„
    """
    # 1. å®šä¹‰è·¯å¾„
    fold_suffix = f"_fold_{fold}" if fold is not None else ""
    pkl_train_path = join(FEATURE_ROOT, dataset_name, f"train{fold_suffix}.pkl")
    pkl_test_path = join(FEATURE_ROOT, dataset_name, f"test{fold_suffix}.pkl")
    final_train_path = join(FEATURE_ROOT, dataset_name, f"train{fold_suffix}_with_features.pkl")
    final_test_path = join(FEATURE_ROOT, dataset_name, f"test{fold_suffix}_with_features.pkl")
    
    # åˆ›å»ºæ•°æ®é›†ç›®å½•
    os.makedirs(join(FEATURE_ROOT, dataset_name), exist_ok=True)
    
    # 2. CSVè½¬PKL
    df_train = csv_to_pkl(csv_train_path, pkl_train_path)
    df_test = csv_to_pkl(csv_test_path, pkl_test_path)
    
    # 3. æå–å¹¶æ·»åŠ ç‰¹å¾ï¼ˆå¦‚æœå°šæœªå¤„ç†ï¼‰
    if not os.path.exists(final_train_path) or not os.path.exists(final_test_path):
        print(f"â„¹ï¸ å¼€å§‹ä¸º{dataset_name}{fold_suffix}æå–ç‰¹å¾...")
        
        # æå–ESMç‰¹å¾
        print("æå–è®­ç»ƒé›†ESMç‰¹å¾...")
        esm_train = extract_esm_features(df_train)
        print("æå–æµ‹è¯•é›†ESMç‰¹å¾...")
        esm_test = extract_esm_features(df_test)
        
        # æå–DRFPç‰¹å¾
        print("æå–è®­ç»ƒé›†DRFPç‰¹å¾...")
        drfp_train = extract_drfp_features(df_train)
        print("æå–æµ‹è¯•é›†DRFPç‰¹å¾...")
        drfp_test = extract_drfp_features(df_test)
        
        # æ·»åŠ ç‰¹å¾åˆ°DataFrame
        df_train["ESM1b"] = esm_train.tolist()
        df_test["ESM1b"] = esm_test.tolist()
        df_train["drfp"] = drfp_train.tolist()
        df_test["drfp"] = drfp_test.tolist()
        
        # ä¿å­˜æœ€ç»ˆå¸¦ç‰¹å¾çš„PKL
        df_train.to_pickle(final_train_path)
        df_test.to_pickle(final_test_path)
        print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œä¿å­˜è‡³: {final_train_path} å’Œ {final_test_path}")
    else:
        print(f"â„¹ï¸ ç‰¹å¾å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½: {final_train_path} å’Œ {final_test_path}")
    
    return final_train_path, final_test_path


# -------------------------- 5. æ¨¡å‹è®­ç»ƒå‡½æ•° --------------------------
def train_models(train_path, test_path, dataset_name, fold=None):
    """ä½¿ç”¨å¸¦ç‰¹å¾çš„PKLæ•°æ®è®­ç»ƒ4ç§XGBoostæ¨¡å‹"""
    # 1. åŠ è½½æ•°æ®
    data_train = pd.read_pickle(train_path)
    data_test = pd.read_pickle(test_path)
    
    # ç¡®ä¿ç›®æ ‡åˆ—å­˜åœ¨
    if TARGET_COL not in data_train.columns or TARGET_COL not in data_test.columns:
        raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘ç›®æ ‡åˆ—: {TARGET_COL}")
    
    # 2. å‡†å¤‡æ¨¡å‹ä¿å­˜ç›®å½•
    fold_suffix = f"_fold_{fold}" if fold is not None else ""
    model_dir = join(MODEL_ROOT, dataset_name)
    os.makedirs(model_dir, exist_ok=True)
    print(f"ğŸ“‚ æ¨¡å‹å°†ä¿å­˜è‡³: {model_dir}")
    
    # 3. æå–ç‰¹å¾å’Œç›®æ ‡å€¼
    # è®­ç»ƒé›†
    train_ESM1b = np.array(list(data_train["ESM1b"]))
    train_drfp = np.array(list(data_train["drfp"]))
    train_Y = np.array(list(data_train[TARGET_COL].dropna()))
    # æµ‹è¯•é›†
    test_ESM1b = np.array(list(data_test["ESM1b"]))
    test_drfp = np.array(list(data_test["drfp"]))
    test_Y = np.array(list(data_test[TARGET_COL].dropna()))
    
    # è¿‡æ»¤ç©ºå€¼
    valid_train = ~np.isnan(train_Y)
    valid_test = ~np.isnan(test_Y)
    train_ESM1b = train_ESM1b[valid_train]
    train_drfp = train_drfp[valid_train]
    train_Y = train_Y[valid_train]
    test_ESM1b = test_ESM1b[valid_test]
    test_drfp = test_drfp[valid_test]
    test_Y = test_Y[valid_test]


    # -------------------------- æ¨¡å‹1: ESM1bç‰¹å¾ --------------------------
    print("\n----- è®­ç»ƒESM1bç‰¹å¾æ¨¡å‹ -----")
    param = {
        'learning_rate': 0.2831145406836757,
        'max_delta_step': 0.07686715986169101,
        'max_depth': int(np.round(4.96836783761305)),
        'min_child_weight': 6.905400087083855,
        'reg_alpha': 1.717314107718892,
        'reg_lambda': 2.470354543039016,
        'objective': 'reg:squarederror'
    }
    num_round = 313  # å–æ•´å¤„ç†
    
    dtrain = xgb.DMatrix(train_ESM1b, label=train_Y)
    dtest = xgb.DMatrix(test_ESM1b, label=test_Y)
    
    bst = xgb.train(param, dtrain, num_round, verbose_eval=False)
    esm_model_path = join(model_dir, f"xgb_esm1b{fold_suffix}.model")
    bst.save_model(esm_model_path)
    
    # è¯„ä¼°
    y_pred_esm = bst.predict(dtest)
    mse_esm = np.mean(np.square(test_Y - y_pred_esm))
    r2_esm = r2_score(test_Y, y_pred_esm)
    pearson_esm = stats.pearsonr(test_Y, y_pred_esm)[0]
    
    print(f"ESM1bæ¨¡å‹ä¿å­˜è‡³: {esm_model_path}")
    print(f"æ€§èƒ½: Pearson={pearson_esm:.4f}, MSE={mse_esm:.4f}, RÂ²={r2_esm:.4f}")


    # -------------------------- æ¨¡å‹2: DRFPç‰¹å¾ --------------------------
    print("\n----- è®­ç»ƒDRFPç‰¹å¾æ¨¡å‹ -----")
    param = {
        'learning_rate': 0.08987247189322463,
        'max_delta_step': 1.1939737318908727,
        'max_depth': int(np.round(11.268531225242574)),
        'min_child_weight': 2.8172720953826302,
        'reg_alpha': 1.9412226989868904,
        'reg_lambda': 4.950543905603358,
        'objective': 'reg:squarederror'
    }
    num_round = 109  # å–æ•´å¤„ç†
    
    dtrain = xgb.DMatrix(train_drfp, label=train_Y)
    dtest = xgb.DMatrix(test_drfp, label=test_Y)
    
    bst = xgb.train(param, dtrain, num_round, verbose_eval=False)
    drfp_model_path = join(model_dir, f"xgb_drfp{fold_suffix}.model")
    bst.save_model(drfp_model_path)
    
    # è¯„ä¼°
    y_pred_drfp = bst.predict(dtest)
    mse_drfp = np.mean(np.square(test_Y - y_pred_drfp))
    r2_drfp = r2_score(test_Y, y_pred_drfp)
    pearson_drfp = stats.pearsonr(test_Y, y_pred_drfp)[0]
    
    print(f"DRFPæ¨¡å‹ä¿å­˜è‡³: {drfp_model_path}")
    print(f"æ€§èƒ½: Pearson={pearson_drfp:.4f}, MSE={mse_drfp:.4f}, RÂ²={r2_drfp:.4f}")


    # -------------------------- æ¨¡å‹3: ESM1b+DRFPç»„åˆç‰¹å¾ --------------------------
    print("\n----- è®­ç»ƒç»„åˆç‰¹å¾æ¨¡å‹ -----")
    # æ‹¼æ¥ç‰¹å¾
    train_combined = np.concatenate([train_ESM1b, train_drfp], axis=1)
    test_combined = np.concatenate([test_ESM1b, test_drfp], axis=1)
    
    param = {
        'learning_rate': 0.05221672412884108,
        'max_delta_step': 1.0767235463496743,
        'max_depth': int(np.round(11.329014411591299)),
        'min_child_weight': 14.724796449973605,
        'reg_alpha': 2.8295816318634452,
        'reg_lambda': 0.6528469146574993,
        'objective': 'reg:squarederror'
    }
    num_round = 299  # å–æ•´å¤„ç†
    
    dtrain = xgb.DMatrix(train_combined, label=train_Y)
    dtest = xgb.DMatrix(test_combined, label=test_Y)
    
    bst = xgb.train(param, dtrain, num_round, verbose_eval=False)
    combined_model_path = join(model_dir, f"xgb_combined{fold_suffix}.model")
    bst.save_model(combined_model_path)
    
    # è¯„ä¼°
    y_pred_combined = bst.predict(dtest)
    mse_combined = np.mean(np.square(test_Y - y_pred_combined))
    r2_combined = r2_score(test_Y, y_pred_combined)
    pearson_combined = stats.pearsonr(test_Y, y_pred_combined)[0]
    
    print(f"ç»„åˆæ¨¡å‹ä¿å­˜è‡³: {combined_model_path}")
    print(f"æ€§èƒ½: Pearson={pearson_combined:.4f}, MSE={mse_combined:.4f}, RÂ²={r2_combined:.4f}")


    # -------------------------- æ¨¡å‹4: å‡å€¼èåˆ --------------------------
    print("\n----- è®¡ç®—å‡å€¼èåˆç»“æœ -----")
    y_pred_mean = (y_pred_esm + y_pred_drfp) / 2
    
    # è¯„ä¼°
    mse_mean = np.mean(np.square(test_Y - y_pred_mean))
    r2_mean = r2_score(test_Y, y_pred_mean)
    pearson_mean = stats.pearsonr(test_Y, y_pred_mean)[0]
    
    # ä¿å­˜èåˆç»“æœ
    mean_result_path = join(model_dir, f"mean_fusion{fold_suffix}.pkl")
    with open(mean_result_path, "wb") as f:
        pickle.dump({
            "y_true": test_Y,
            "y_pred_esm": y_pred_esm,
            "y_pred_drfp": y_pred_drfp,
            "y_pred_mean": y_pred_mean
        }, f)
    
    print(f"å‡å€¼èåˆç»“æœä¿å­˜è‡³: {mean_result_path}")
    print(f"æ€§èƒ½: Pearson={pearson_mean:.4f}, MSE={mse_mean:.4f}, RÂ²={r2_mean:.4f}")


# -------------------------- 6. æ•°æ®é›†è®­ç»ƒæµç¨‹ --------------------------
def train_catpred():
    """è®­ç»ƒCatPredåŸºç¡€æ•°æ®é›†"""
    print("\n" + "="*50)
    print("å¼€å§‹è®­ç»ƒ CatPred åŸºç¡€æ•°æ®é›†")
    print("="*50)
    
    # æ•°æ®è·¯å¾„
    train_csv = join(DATA_ROOT, "data", "catpred", "train")
    test_csv = join(DATA_ROOT, "data", "catpred", "test")
    
    # é¢„å¤„ç†æ•°æ®
    train_path, test_path = preprocess_data(
        train_csv, test_csv, 
        dataset_name="catpred"
    )
    
    # è®­ç»ƒæ¨¡å‹
    train_models(train_path, test_path, dataset_name="catpred")


def train_cold_enzyme():
    """è®­ç»ƒCold-Enzymeæ•°æ®é›†ï¼ˆ1-5æŠ˜ï¼‰"""
    print("\n" + "="*50)
    print("å¼€å§‹è®­ç»ƒ Cold-Enzyme æ•°æ®é›†ï¼ˆ1-5æŠ˜ï¼‰")
    print("="*50)
    
    for fold in range(1, 6):  # 1-5æŠ˜
        print(f"\n" + "-"*40)
        print(f"å¤„ç† Cold-Enzyme ç¬¬ {fold} æŠ˜")
        print("-"*40)
        
        # æ•°æ®è·¯å¾„
        train_csv = join(DATA_ROOT, "data", "turnup", "cold_enzyme", 
                        f"kcat_train_fold_{fold}_en.csv")
        test_csv = join(DATA_ROOT, "data", "turnup", "cold_enzyme", 
                       f"kcat_val_fold_{fold}_en.csv")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(train_csv) or not os.path.exists(test_csv):
            print(f"âŒ ç¬¬ {fold} æŠ˜æ–‡ä»¶ç¼ºå¤±ï¼Œè·³è¿‡")
            continue
        
        # é¢„å¤„ç†æ•°æ®
        train_path, test_path = preprocess_data(
            train_csv, test_csv, 
            dataset_name="cold_enzyme",
            fold=fold
        )
        
        # è®­ç»ƒæ¨¡å‹
        train_models(train_path, test_path, dataset_name="cold_enzyme", fold=fold)


def train_cold_reaction():
    """è®­ç»ƒCold-Reactionæ•°æ®é›†ï¼ˆ1-5æŠ˜ï¼‰"""
    print("\n" + "="*50)
    print("å¼€å§‹è®­ç»ƒ Cold-Reaction æ•°æ®é›†ï¼ˆ1-5æŠ˜ï¼‰")
    print("="*50)
    
    for fold in range(1, 6):  # 1-5æŠ˜
        print(f"\n" + "-"*40)
        print(f"å¤„ç† Cold-Reaction ç¬¬ {fold} æŠ˜")
        print("-"*40)
        
        # æ•°æ®è·¯å¾„
        train_csv = join(DATA_ROOT, "data", "turnup", "cold_reaction", 
                        f"kcat_train_fold_{fold}.csv")
        test_csv = join(DATA_ROOT, "data", "turnup", "cold_reaction", 
                       f"kcat_val_fold_{fold}.csv")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(train_csv) or not os.path.exists(test_csv):
            print(f"âŒ ç¬¬ {fold} æŠ˜æ–‡ä»¶ç¼ºå¤±ï¼Œè·³è¿‡")
            continue
        
        # é¢„å¤„ç†æ•°æ®
        train_path, test_path = preprocess_data(
            train_csv, test_csv, 
            dataset_name="cold_reaction",
            fold=fold
        )
        
        # è®­ç»ƒæ¨¡å‹
        train_models(train_path, test_path, dataset_name="cold_reaction", fold=fold)


def train_warm():
    """è®­ç»ƒWarmæ•°æ®é›†ï¼ˆ0-4æŠ˜ï¼‰"""
    print("\n" + "="*50)
    print("å¼€å§‹è®­ç»ƒ Warm æ•°æ®é›†ï¼ˆ0-4æŠ˜ï¼‰")
    print("="*50)
    
    for fold in range(0, 5):  # 0-4æŠ˜
        print(f"\n" + "-"*40)
        print(f"å¤„ç† Warm ç¬¬ {fold} æŠ˜")
        print("-"*40)
        
        # æ•°æ®è·¯å¾„
        train_csv = join(DATA_ROOT, "data", "turnup", "warm", 
                        f"kcat_train_data_{fold}.csv")
        test_csv = join(DATA_ROOT, "data", "turnup", "warm", 
                       f"kcat_test_data_{fold}.csv")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(train_csv) or not os.path.exists(test_csv):
            print(f"âŒ ç¬¬ {fold} æŠ˜æ–‡ä»¶ç¼ºå¤±ï¼Œè·³è¿‡")
            continue
        
        # é¢„å¤„ç†æ•°æ®
        train_path, test_path = preprocess_data(
            train_csv, test_csv, 
            dataset_name="warm",
            fold=fold
        )
        
        # è®­ç»ƒæ¨¡å‹
        train_models(train_path, test_path, dataset_name="warm", fold=fold)


# -------------------------- ä¸»å‡½æ•° --------------------------
if __name__ == "__main__":
    train_catpred()          # CatPredåŸºç¡€æ•°æ®é›†
    train_cold_enzyme()      # Cold-Enzymeï¼ˆ1-5æŠ˜ï¼‰
    train_cold_reaction()    # Cold-Reactionï¼ˆ1-5æŠ˜ï¼‰
    train_warm()             # Warmï¼ˆ0-4æŠ˜ï¼‰
    
    print("\n" + "="*50)
    print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
    print(f"æ¨¡å‹ä¿å­˜æ ¹ç›®å½•: {MODEL_ROOT}")
    print(f"ç‰¹å¾ä¿å­˜æ ¹ç›®å½•: {FEATURE_ROOT}")
    print("="*50)
